{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from models import SRCNN\n",
        "from datasets import TrainDataset, EvalDataset\n",
        "from utils import AverageMeter, calc_psnr\n",
        "\n",
        "import mlflow\n",
        "import numpy as np\n",
        "from azureml.core import Workspace\n",
        "from mlflow import MlflowClient"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1669786389049
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.from_config()\n",
        "experiment_name = \"isr_cs5412\"\n",
        "\n",
        "# set up MLflow to track the metrics\n",
        "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
        "mlflow.set_experiment(experiment_name)\n",
        "mlflow.autolog()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2022/11/30 05:33:11 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n2022/11/30 05:33:11 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n2022/11/30 05:33:11 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1669786391323
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1669786391430
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_file  = \"data/91-image_x4.h5\"\n",
        "eval_file   = \"data/Set5_x4.h5\"\n",
        "outputs_dir = \"outputs\"\n",
        "scale       = 4\n",
        "lr          = 1e-4\n",
        "B           = 16\n",
        "E           = 50\n",
        "n_workers   = 3\n",
        "seed        = 3\n"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1669786391528
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if __name__ == '__main__':\n",
        "#     parser = argparse.ArgumentParser()\n",
        "#     parser.add_argument('--train-file',     default=\"data/91-image_x4.h5\",  type=str)\n",
        "#     parser.add_argument('--eval-file',      default=\"data/Set5_x4.h5\",      type=str)\n",
        "#     parser.add_argument('--outputs-dir',    default=\"outputs\",              type=str)\n",
        "#     parser.add_argument('--scale',          default=3,                      type=int)\n",
        "#     parser.add_argument('--lr',             default=1e-4,                   type=float)\n",
        "#     parser.add_argument('--batch-size',     default=16,                     type=int)\n",
        "#     parser.add_argument('--num-epochs',     default=400,                    type=int)\n",
        "#     parser.add_argument('--num-workers',    default=8,                      type=int)\n",
        "#     parser.add_argument('--seed',           default=114,                    type=int)\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "outputs_dir = os.path.join(outputs_dir, 'x{}'.format(scale))\n",
        "\n",
        "if not os.path.exists(outputs_dir):\n",
        "    os.makedirs(outputs_dir)\n",
        "\n",
        "cudnn.benchmark = True\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "model = SRCNN().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam([\n",
        "    {'params': model.conv1.parameters()},\n",
        "    {'params': model.conv2.parameters()},\n",
        "    {'params': model.conv3.parameters(), 'lr': lr * 0.1}\n",
        "], lr=lr)\n",
        "\n",
        "train_dataset = TrainDataset(train_file)\n",
        "train_dataloader = DataLoader(dataset=train_dataset,\n",
        "                                batch_size=B,\n",
        "                                shuffle=True,\n",
        "                                num_workers=n_workers,\n",
        "                                pin_memory=True,\n",
        "                                drop_last=True)\n",
        "eval_dataset = EvalDataset(eval_file)\n",
        "eval_dataloader = DataLoader(dataset=eval_dataset, batch_size=1)\n",
        "\n",
        "\n",
        "best_weights = copy.deepcopy(model.state_dict())\n",
        "best_epoch = 0\n",
        "best_psnr = 0.0\n",
        "\n",
        "for epoch in range(E):\n",
        "    model.train()\n",
        "    epoch_losses = AverageMeter()\n",
        "\n",
        "    with tqdm(total=(len(train_dataset) - len(train_dataset) % B)) as t:\n",
        "        t.set_description('epoch: {}/{}'.format(epoch, E - 1))\n",
        "\n",
        "        for data in train_dataloader:\n",
        "            inputs, labels = data\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            preds = model(inputs)\n",
        "\n",
        "            loss = criterion(preds, labels)\n",
        "\n",
        "            epoch_losses.update(loss.item(), len(inputs))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            t.set_postfix(loss='{:.6f}'.format(epoch_losses.avg))\n",
        "            t.update(len(inputs))\n",
        "\n",
        "    torch.save(model.state_dict(), os.path.join(outputs_dir, 'epoch_{}.pth'.format(epoch)))\n",
        "\n",
        "    model.eval()\n",
        "    epoch_psnr = AverageMeter()\n",
        "\n",
        "    for data in eval_dataloader:\n",
        "        inputs, labels = data\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            preds = model(inputs).clamp(0.0, 1.0)\n",
        "\n",
        "        epoch_psnr.update(calc_psnr(preds, labels), len(inputs))\n",
        "\n",
        "    print('eval psnr: {:.2f}'.format(epoch_psnr.avg))\n",
        "\n",
        "    if epoch_psnr.avg > best_psnr:\n",
        "        best_epoch = epoch\n",
        "        best_psnr = epoch_psnr.avg\n",
        "        best_weights = copy.deepcopy(model.state_dict())\n",
        "\n",
        "print('best epoch: {}, psnr: {:.2f}'.format(best_epoch, best_psnr))\n",
        "torch.save(best_weights, os.path.join(outputs_dir, 'best.pth'))\n",
        "\n",
        "\n",
        "with mlflow.start_run() as run:\n",
        "    mlflow.pytorch.log_model(model, \"model\")\n",
        "\n",
        "    # convert to scripted model and log the model\n",
        "    scripted_pytorch_model = torch.jit.script(model)\n",
        "    mlflow.pytorch.log_model(scripted_pytorch_model, \"scripted_model\")\n",
        "\n",
        "# Fetch the logged model artifacts\n",
        "print(\"run_id: {}\".format(run.info.run_id))\n",
        "for artifact_path in [\"model/data\", \"scripted_model/data\"]:\n",
        "    artifacts = [f.path for f in MlflowClient().list_artifacts(run.info.run_id,\n",
        "                artifact_path)]\n",
        "    print(\"artifacts: {}\".format(artifacts))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "epoch: 0/49: 100%|██████████| 21760/21760 [00:28<00:00, 759.52it/s, loss=0.003777] \nepoch: 1/49: 100%|██████████| 21760/21760 [00:24<00:00, 880.38it/s, loss=0.002125] \nepoch: 2/49: 100%|██████████| 21760/21760 [00:14<00:00, 1469.08it/s, loss=0.002073]\nepoch: 3/49: 100%|██████████| 21760/21760 [00:15<00:00, 1445.08it/s, loss=0.002045]\nepoch: 4/49: 100%|██████████| 21760/21760 [00:14<00:00, 1482.87it/s, loss=0.002028]\nepoch: 5/49: 100%|██████████| 21760/21760 [00:15<00:00, 1448.32it/s, loss=0.002012]\nepoch: 6/49: 100%|██████████| 21760/21760 [00:14<00:00, 1462.98it/s, loss=0.002001]\nepoch: 7/49: 100%|██████████| 21760/21760 [00:14<00:00, 1489.53it/s, loss=0.001991]\nepoch: 8/49: 100%|██████████| 21760/21760 [00:14<00:00, 1454.03it/s, loss=0.001983]\nepoch: 9/49: 100%|██████████| 21760/21760 [00:14<00:00, 1464.28it/s, loss=0.001974]\nepoch: 10/49: 100%|██████████| 21760/21760 [00:14<00:00, 1479.91it/s, loss=0.001967]\nepoch: 11/49: 100%|██████████| 21760/21760 [00:14<00:00, 1460.51it/s, loss=0.001959]\nepoch: 12/49: 100%|██████████| 21760/21760 [00:14<00:00, 1477.65it/s, loss=0.001953]\nepoch: 13/49: 100%|██████████| 21760/21760 [00:14<00:00, 1458.26it/s, loss=0.001947]\nepoch: 14/49: 100%|██████████| 21760/21760 [00:14<00:00, 1465.61it/s, loss=0.001942]\nepoch: 15/49: 100%|██████████| 21760/21760 [00:14<00:00, 1461.04it/s, loss=0.001936]\nepoch: 16/49: 100%|██████████| 21760/21760 [00:15<00:00, 1449.14it/s, loss=0.001931]\nepoch: 17/49: 100%|██████████| 21760/21760 [00:14<00:00, 1476.01it/s, loss=0.001925]\nepoch: 18/49: 100%|██████████| 21760/21760 [00:14<00:00, 1487.98it/s, loss=0.001922]\nepoch: 19/49: 100%|██████████| 21760/21760 [00:14<00:00, 1458.15it/s, loss=0.001918]\nepoch: 20/49: 100%|██████████| 21760/21760 [00:14<00:00, 1453.67it/s, loss=0.001915]\nepoch: 21/49: 100%|██████████| 21760/21760 [00:14<00:00, 1455.71it/s, loss=0.001910]\nepoch: 22/49: 100%|██████████| 21760/21760 [00:14<00:00, 1459.97it/s, loss=0.001907]\nepoch: 23/49: 100%|██████████| 21760/21760 [00:14<00:00, 1470.79it/s, loss=0.001904]\nepoch: 24/49: 100%|██████████| 21760/21760 [00:15<00:00, 1446.50it/s, loss=0.001901]\nepoch: 25/49: 100%|██████████| 21760/21760 [00:15<00:00, 1442.92it/s, loss=0.001899]\nepoch: 26/49: 100%|██████████| 21760/21760 [00:14<00:00, 1459.14it/s, loss=0.001896]\nepoch: 27/49: 100%|██████████| 21760/21760 [00:15<00:00, 1444.09it/s, loss=0.001893]\nepoch: 28/49: 100%|██████████| 21760/21760 [00:14<00:00, 1456.65it/s, loss=0.001890]\nepoch: 29/49: 100%|██████████| 21760/21760 [00:14<00:00, 1471.56it/s, loss=0.001888]\nepoch: 30/49: 100%|██████████| 21760/21760 [00:15<00:00, 1447.91it/s, loss=0.001886]\nepoch: 31/49: 100%|██████████| 21760/21760 [00:14<00:00, 1455.77it/s, loss=0.001883]\nepoch: 32/49: 100%|██████████| 21760/21760 [00:15<00:00, 1438.28it/s, loss=0.001881]\nepoch: 33/49: 100%|██████████| 21760/21760 [00:15<00:00, 1447.68it/s, loss=0.001880]\nepoch: 34/49: 100%|██████████| 21760/21760 [00:14<00:00, 1451.03it/s, loss=0.001879]\nepoch: 35/49: 100%|██████████| 21760/21760 [00:14<00:00, 1456.46it/s, loss=0.001875]\nepoch: 36/49: 100%|██████████| 21760/21760 [00:14<00:00, 1454.54it/s, loss=0.001875]\nepoch: 37/49: 100%|██████████| 21760/21760 [00:14<00:00, 1467.64it/s, loss=0.001872]\nepoch: 38/49: 100%|██████████| 21760/21760 [00:14<00:00, 1454.42it/s, loss=0.001870]\nepoch: 39/49: 100%|██████████| 21760/21760 [00:14<00:00, 1467.56it/s, loss=0.001869]\nepoch: 40/49: 100%|██████████| 21760/21760 [00:15<00:00, 1444.68it/s, loss=0.001868]\nepoch: 41/49: 100%|██████████| 21760/21760 [00:15<00:00, 1435.49it/s, loss=0.001866]\nepoch: 42/49: 100%|██████████| 21760/21760 [00:15<00:00, 1429.60it/s, loss=0.001864]\nepoch: 43/49: 100%|██████████| 21760/21760 [00:14<00:00, 1452.57it/s, loss=0.001862]\nepoch: 44/49: 100%|██████████| 21760/21760 [00:14<00:00, 1457.52it/s, loss=0.001861]\nepoch: 45/49: 100%|██████████| 21760/21760 [00:14<00:00, 1475.70it/s, loss=0.001861]\nepoch: 46/49: 100%|██████████| 21760/21760 [00:14<00:00, 1472.73it/s, loss=0.001859]\nepoch: 47/49: 100%|██████████| 21760/21760 [00:14<00:00, 1487.02it/s, loss=0.001856]\nepoch: 48/49: 100%|██████████| 21760/21760 [00:14<00:00, 1465.48it/s, loss=0.001856]\nepoch: 49/49: 100%|██████████| 21760/21760 [00:14<00:00, 1458.66it/s, loss=0.001856]\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "eval psnr: 29.25\neval psnr: 29.48\neval psnr: 29.58\neval psnr: 29.64\neval psnr: 29.68\neval psnr: 29.75\neval psnr: 29.75\neval psnr: 29.83\neval psnr: 29.81\neval psnr: 29.83\neval psnr: 29.88\neval psnr: 29.90\neval psnr: 29.79\neval psnr: 29.91\neval psnr: 29.87\neval psnr: 29.91\neval psnr: 29.91\neval psnr: 29.91\neval psnr: 29.95\neval psnr: 29.69\neval psnr: 29.99\neval psnr: 29.99\neval psnr: 30.02\neval psnr: 30.02\neval psnr: 30.02\neval psnr: 30.03\neval psnr: 30.03\neval psnr: 30.02\neval psnr: 30.01\neval psnr: 29.95\neval psnr: 30.07\neval psnr: 30.06\neval psnr: 30.01\neval psnr: 30.07\neval psnr: 30.07\neval psnr: 30.08\neval psnr: 30.05\neval psnr: 30.08\neval psnr: 30.09\neval psnr: 30.08\neval psnr: 30.11\neval psnr: 30.06\neval psnr: 30.10\neval psnr: 30.11\neval psnr: 30.12\neval psnr: 30.13\neval psnr: 30.11\neval psnr: 30.10\neval psnr: 30.14\neval psnr: 30.10\nbest epoch: 48, psnr: 30.14\nrun_id: 579a43be-2ec1-457e-b4ed-b6490fd14c24\nartifacts: ['model/data/model.pth', 'model/data/pickle_module_info.txt']\nartifacts: ['scripted_model/data/model.pth', 'scripted_model/data/pickle_module_info.txt']\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1669787342122
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run.info.run_id"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "'579a43be-2ec1-457e-b4ed-b6490fd14c24'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669787446475
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# register the model\n",
        "model_uri = \"runs:/{}/model\".format(run.info.run_id)\n",
        "model = mlflow.register_model(model_uri, \"isr_srcnn_x4\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Successfully registered model 'isr_srcnn_x4'.\n2022/11/30 05:50:55 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: isr_srcnn_x4, version 1\nCreated version '1' of model 'isr_srcnn_x4'.\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669787455333
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = mlflow.pytorch.load_model(model_uri=model_uri)\n",
        "model"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "SRCNN(\n  (conv1): Conv2d(1, 64, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n  (conv2): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n  (conv3): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n  (relu): ReLU(inplace=True)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669790043809
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import PIL.Image as pil_image\n",
        "\n",
        "from models import SRCNN\n",
        "from utils import convert_rgb_to_ycbcr, convert_ycbcr_to_rgb, calc_psnr\n",
        "\n",
        "image_url = \"https://raw.githubusercontent.com/Coloquinte/torchSR/v1.0.2/doc/example_small.png\"\n",
        "r = requests.get(image_url, stream=True)\n",
        "r.raw"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "<urllib3.response.HTTPResponse at 0x7f2bbf98f670>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669790599517
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r.status_code"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "200"
          },
          "metadata": {}
        }
      ],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669790613235
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "image_raw = Image.open(BytesIO(r.content)).convert('RGB')\n",
        "image_raw.save(f'thumbnails{os.sep}butterfly.jpg')\n",
        "image_file = f'thumbnails{os.sep}butterfly.jpg'"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669791274655
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = pil_image.open(image_file).convert('RGB')\n",
        "# image = Image.open(BytesIO(r.content)).convert('RGB')\n",
        "\n",
        "image_width = (image.width // scale) * scale\n",
        "image_height = (image.height // scale) * scale\n",
        "image = image.resize((image_width, image_height), resample=pil_image.BICUBIC)\n",
        "image = image.resize((image.width // scale, image.height // scale), resample=pil_image.BICUBIC)\n",
        "image = image.resize((image.width * scale, image.height * scale), resample=pil_image.BICUBIC)\n",
        "image.save(image_file.replace('.', '_bicubic_x{}.'.format(scale)))\n",
        "\n",
        "image = np.array(image).astype(np.float32)\n",
        "ycbcr = convert_rgb_to_ycbcr(image)\n",
        "\n",
        "y = ycbcr[..., 0]\n",
        "y /= 255.\n",
        "y = torch.from_numpy(y).to(device)\n",
        "y = y.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds = model(y).clamp(0.0, 1.0)\n",
        "\n",
        "psnr = calc_psnr(y, preds)\n",
        "print('PSNR: {:.2f}'.format(psnr))\n",
        "\n",
        "preds = preds.mul(255.0).cpu().numpy().squeeze(0).squeeze(0)\n",
        "\n",
        "output = np.array([preds, ycbcr[..., 1], ycbcr[..., 2]]).transpose([1, 2, 0])\n",
        "output = np.clip(convert_ycbcr_to_rgb(output), 0.0, 255.0).astype(np.uint8)\n",
        "output = pil_image.fromarray(output)\n",
        "output.save(image_file.replace('.', '_srcnn_x{}.'.format(scale)))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "PSNR: 28.88\n"
        }
      ],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669791278664
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## deployment configs"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create environment for the deploy\n",
        "from azureml.core.environment import Environment\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.core.webservice import AciWebservice\n",
        "\n",
        "# get a curated environment\n",
        "env = Environment.get(\n",
        "    workspace=ws, \n",
        "    # name=\"AzureML-pytorch-1.12.0-ubuntu18.04-py37-cpu-inference\",\n",
        "    name=\"AzureML-ACPT-pytorch-1.12-py38-cuda11.6-gpu\",\n",
        "    version=1\n",
        ")\n",
        "env.inferencing_stack_version='latest'\n",
        "\n",
        "# create deployment config i.e. compute resources\n",
        "aciconfig = AciWebservice.deploy_configuration(\n",
        "    cpu_cores=8,\n",
        "    memory_gb=32,\n",
        "    tags={\"data\": \"91-images_x4\", \"method\": \"srcnn_isr_x4\"},\n",
        "    description=\"Image Super Resolution with SRCNN\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669788191463
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## deploy model "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import uuid\n",
        "from azureml.core.model import InferenceConfig\n",
        "from azureml.core.environment import Environment\n",
        "from azureml.core.model import Model\n",
        "\n",
        "# get the registered model\n",
        "model = Model(ws, \"isr_srcnn_x4\")\n",
        "\n",
        "# create an inference config i.e. the scoring script and environment\n",
        "inference_config = InferenceConfig(entry_script=\"score.py\", environment=env)\n",
        "\n",
        "# deploy the service\n",
        "service_name = \"isr-srcnn-x4-\" + str(uuid.uuid4())[:4]\n",
        "service = Model.deploy(\n",
        "    workspace=ws,\n",
        "    name=service_name,\n",
        "    models=[model],\n",
        "    inference_config=inference_config,\n",
        "    deployment_config=aciconfig,\n",
        ")\n",
        "\n",
        "service.wait_for_deployment(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# send raw HTTP request to test the web service.\n",
        "import requests\n",
        "\n",
        "# send a random row from the test set to score\n",
        "random_index = np.random.randint(0, len(X_test) - 1)\n",
        "input_data = '{\"data\": [' + str(list(X_test[random_index])) + \"]}\"\n",
        "\n",
        "headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "resp = requests.post(service.scoring_uri, input_data, headers=headers)\n",
        "\n",
        "print(\"POST to url\", service.scoring_uri)\n",
        "print(\"label:\", y_test[random_index])\n",
        "print(\"prediction:\", resp.text)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}